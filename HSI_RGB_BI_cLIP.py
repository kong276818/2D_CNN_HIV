import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.metrics import accuracy_score
import time
from tqdm import tqdm

# ===============================
# âœ… 1. ì„¤ì • í´ë˜ìŠ¤
# ===============================
class Config:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ê²½ë¡œ ì„¤ì •ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    DATA_DIR = "data"
    BATCH_SIZE = 4
    EPOCHS = 20
    LEARNING_RATE = 1e-4
    SAVE_PATH = "best_model_hsi_only.pth"  # HSI ì „ìš© ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    VALIDATION_SPLIT = 0.2
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ===============================
# âœ… 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (HSI ì „ìš©)
# ===============================
class HSIDataset(Dataset):
    """HSI(ì´ˆë¶„ê´‘) ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, data_dir):
        try:
            # HSI ë°ì´í„°ë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
            pe_hsi = np.load(os.path.join(data_dir, 'pe_cube_filtered.npy'))
            pp_hsi = np.load(os.path.join(data_dir, 'pp_cube_filtered.npy'))
        except FileNotFoundError as e:
            raise FileNotFoundError(f"âŒ '{e.filename}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. '{data_dir}' í´ë”ì— HSI íŒŒì¼ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

        # ë°ì´í„°ë¥¼ PyTorch í˜•ì‹ (N, C, H, W)ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        pe_hsi = self._to_nchw(pe_hsi, name='pe_hsi')
        pp_hsi = self._to_nchw(pp_hsi, name='pp_hsi')

        # ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ë§ì¶¥ë‹ˆë‹¤.
        min_h = min(pe_hsi.shape[2], pp_hsi.shape[2])
        min_w = min(pe_hsi.shape[3], pp_hsi.shape[3])
        pe_hsi = pe_hsi[:, :, :min_h, :min_w]
        pp_hsi = pp_hsi[:, :, :min_h, :min_w]

        # ë°ì´í„°ì™€ ë ˆì´ë¸”ì„ ê²°í•©í•˜ê³  ì •ê·œí™”í•©ë‹ˆë‹¤.
        self.hsi = np.concatenate([pe_hsi, pp_hsi], axis=0).astype(np.float32)
        pe_labels = np.zeros(pe_hsi.shape[0], dtype=np.int64)
        pp_labels = np.ones(pp_hsi.shape[0], dtype=np.int64)
        self.labels = np.concatenate([pe_labels, pp_labels], axis=0)

        # ì •ê·œí™”
        self.hsi /= (np.max(self.hsi) if np.max(self.hsi) > 0 else 1.0)

        self.hsi_channels = self.hsi.shape[1]
        print(f"âœ… HSI ë°ì´í„° ë¡œë“œ ì™„ë£Œ: HSI {self.hsi.shape}, Labels {self.labels.shape}")

    def _to_nchw(self, arr, name=''):
        """Numpy ë°°ì—´ì„ PyTorchê°€ ì„ í˜¸í•˜ëŠ” (N, C, H, W) í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        if arr.ndim == 3: arr = arr[np.newaxis, ...]
        if arr.ndim != 4: raise ValueError(f"âŒ {name}ì€ 4ì°¨ì› ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ í˜•íƒœ: {arr.shape}")
        
        # ì±„ë„ì´ ë§ˆì§€ë§‰ ì°¨ì›ì— ìˆì„ ê²½ìš° (N, H, W, C) -> (N, C, H, W)
        if arr.shape[-1] < arr.shape[1]:
             arr = np.transpose(arr, (0, 3, 1, 2))
        return arr

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        # HSI ë°ì´í„°ì™€ ë ˆì´ë¸”ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.
        return (torch.from_numpy(self.hsi[idx]), torch.tensor(self.labels[idx], dtype=torch.long))

# ===============================
# âœ… 3. ëª¨ë¸ ì •ì˜ (HSI ì „ìš©)
# ===============================
class HSI_Classifier(nn.Module):
    """HSI íŠ¹ì§•ë§Œìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” CNN ëª¨ë¸"""
    def __init__(self, hsi_channels, num_classes=2):
        super().__init__()
        # HSI íŠ¹ì§• ì¶”ì¶œì„ ìœ„í•œ CNN ë¸Œëœì¹˜
        self.feature_extractor = nn.Sequential(
            nn.Conv2d(hsi_channels, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
            nn.AdaptiveAvgPool2d(1)
        )
        # ìµœì¢… ë¶„ë¥˜ë¥¼ ìœ„í•œ ë¶„ë¥˜ê¸°
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.5),
            nn.Linear(32, num_classes)
        )

    def forward(self, hsi):
        features = self.feature_extractor(hsi)
        output = self.classifier(features)
        return output

# ===============================
# âœ… 4. í•™ìŠµ ë° ê²€ì¦ í•¨ìˆ˜
# ===============================
def train_one_epoch(model, dataloader, criterion, optimizer, device):
    """í•œ ì—í¬í¬ ë™ì•ˆ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤."""
    model.train()
    total_loss = 0
    all_preds, all_labels = [], []

    for hsi, labels in tqdm(dataloader, desc="Training"):
        hsi, labels = hsi.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(hsi) # ëª¨ë¸ì— HSI ë°ì´í„°ë§Œ ì „ë‹¬
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        _, preds = torch.max(outputs, 1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
    
    # (ìˆ˜ì •) ë°ì´í„°ë¡œë”ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    if len(dataloader) == 0:
        return 0.0, 0.0
        
    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    return avg_loss, accuracy

def validate_one_epoch(model, dataloader, criterion, device):
    """í•œ ì—í¬í¬ ë™ì•ˆ ëª¨ë¸ ì„±ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
    model.eval()
    total_loss = 0
    all_preds, all_labels = [], []

    with torch.no_grad():
        for hsi, labels in tqdm(dataloader, desc="Validation"):
            hsi, labels = hsi.to(device), labels.to(device)
            outputs = model(hsi) # ëª¨ë¸ì— HSI ë°ì´í„°ë§Œ ì „ë‹¬
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    # (ìˆ˜ì •) ë°ì´í„°ë¡œë”ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš°ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    if len(dataloader) == 0:
        print("âš ï¸ ê²½ê³ : ê²€ì¦ ë°ì´í„°ë¡œë”ê°€ ë¹„ì–´ìˆì–´ ê²€ì¦ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return 0.0, 0.0

    avg_loss = total_loss / len(dataloader)
    accuracy = accuracy_score(all_labels, all_preds)
    return avg_loss, accuracy

# ===============================
# âœ… 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ===============================
def main():
    """ì „ì²´ í•™ìŠµ ê³¼ì •ì„ ì¡°ìœ¨í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤."""
    cfg = Config()
    print(f"ğŸš€ Using device: {cfg.DEVICE}")

    # --- ë°ì´í„° ì¤€ë¹„ ---
    full_dataset = HSIDataset(cfg.DATA_DIR)
    
    # ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆì§€ ì•Šì„ ë•Œë§Œ ë¶„í• ì„ ì‹œë„í•©ë‹ˆë‹¤.
    if len(full_dataset) > 0:
        val_size = int(len(full_dataset) * cfg.VALIDATION_SPLIT)
        train_size = len(full_dataset) - val_size
        
        # val_sizeê°€ 0ì´ ë˜ì§€ ì•Šë„ë¡ ìµœì†Œ 1ê°œëŠ” í• ë‹¹ (ë‹¨, ì „ì²´ ìƒ˜í”Œì´ 1ê°œ ì´ìƒì¼ ë•Œ)
        if train_size > 0 and val_size == 0:
            val_size = 1
            train_size = len(full_dataset) - val_size

        if train_size <= 0 or val_size <= 0:
            train_dataset, val_dataset = full_dataset, None # í•œ ìª½ì´ 0ì´ë©´ ë¶„í• í•˜ì§€ ì•ŠìŒ
            print(f"âš ï¸ ê²½ê³ : ë°ì´í„°ì…‹ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ì•„({len(full_dataset)}ê°œ) í•™ìŠµ/ê²€ì¦ ë¶„í• ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
    else:
        train_dataset, val_dataset = full_dataset, None

    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True)
    # val_datasetì´ Noneì´ ì•„ë‹ ê²½ìš°ì—ë§Œ DataLoader ìƒì„±
    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False) if val_dataset else []
    
    print(f"Split: {len(train_dataset)} train, {len(val_dataset) if val_dataset else 0} validation samples.")

    # --- ëª¨ë¸, ì†ì‹¤í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™” ---
    model = HSI_Classifier(hsi_channels=full_dataset.hsi_channels).to(cfg.DEVICE)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE)

    # --- í•™ìŠµ ë£¨í”„ ---
    best_val_acc = 0.0
    start_time = time.time()

    for epoch in range(cfg.EPOCHS):
        print(f"\n--- Epoch {epoch+1}/{cfg.EPOCHS} ---")
        
        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, cfg.DEVICE)
        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, cfg.DEVICE)

        print(f"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}")

        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), cfg.SAVE_PATH)
            print(f"ğŸ‰ New best model saved with accuracy: {best_val_acc:.4f} to {cfg.SAVE_PATH}")

    total_time = time.time() - start_time
    print(f"\nâœ¨ Training complete in {total_time:.2f}s. Best validation accuracy: {best_val_acc:.4f}")

def prepare_dummy_data():
    """ì‹¤í–‰ì— í•„ìš”í•œ ë”ë¯¸ HSI ë°ì´í„°ì™€ í´ë”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if not os.path.exists("data"):
        print("â„¹ï¸ 'data' í´ë”ì™€ ë”ë¯¸ HSI ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ë¡œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤.")
        os.makedirs("data")
        dummy_pe_hsi = np.random.rand(10, 100, 100, 361).astype(np.float32)
        dummy_pp_hsi = np.random.rand(10, 100, 100, 361).astype(np.float32)
        np.save("data/pe_cube_filtered.npy", dummy_pe_hsi)
        np.save("data/pp_cube_filtered.npy", dummy_pp_hsi)

if __name__ == "__main__":
    prepare_dummy_data()
    try:
        main()
    except (ValueError, FileNotFoundError) as e:
        print(f"\n--- ğŸš¨ ì˜¤ë¥˜ ë°œìƒ! ---")
        print(e)
        print("\n--- ğŸ’¡ í•´ê²° ë°©ë²• ---")
        print("1. 'data' í´ë” ì•ˆì— 2ê°œì˜ .npy íŒŒì¼(pe_cube_filtered, pp_cube_filtered)ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        print("2. ì˜¤ë¥˜ ë©”ì‹œì§€ì— ì–¸ê¸‰ëœ íŒŒì¼ì´ ì˜¬ë°”ë¥¸ í˜•íƒœ(shape)ë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")